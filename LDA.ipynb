{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape a reduced amount of data from the web using Python client for Wikipedia's API.\n",
    "import wikipedia\n",
    "\n",
    "page_titles = ['Python (programming language)', 'Natural language processing', 'Geography', \n",
    "               'History', 'Knowledge', 'Science', 'Art', 'Politics', 'War', 'Food', 'Mathematics']\n",
    "\n",
    "page_summaries = []\n",
    "for page_title in page_titles:\n",
    "    page_summaries.append(wikipedia.WikipediaPage(title=page_title).summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a summary:\n",
      "\"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language co...\"\n"
     ]
    }
   ],
   "source": [
    "print('Example of a summary:\\n\"{}...\"'.format(page_summaries[0][0:250]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/christo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/christo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/christo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "# Tokenize summaries. This includes downloading the relevant pre-proccessing elements for NLTK.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def summary_preprocessing(summary: str) -> list:\n",
    "    '''\n",
    "    Performs preprocessing including lower-casing, dropping non-alphabetic chars,  \n",
    "    dropping stopwords, stemming and lemmatizing.\n",
    "    '''\n",
    "    words = nltk.word_tokenize(summary)\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [porter.stem(word) for word in words]\n",
    "    words = \" \".join(lemma.lemmatize(word) for word in words)\n",
    "    return words\n",
    "\n",
    "tokanized_page_summaries = []\n",
    "for summary in page_summaries:\n",
    "    tokanized_page_summaries.append(summary_preprocessing(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a tokanized summary:\n",
      "python interpret program languag creat guido van rossum first releas python design philosophi emphas...\n"
     ]
    }
   ],
   "source": [
    "print('Example of a tokanized summary:\\n{}...'.format(tokanized_page_summaries[0][0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfrom summaries into a summary-word matrix\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "processed_summaries = [summary_preprocessing(summary).split() for summary in page_summaries]        \n",
    "dictionary = corpora.Dictionary(processed_summaries)\n",
    "summary_word_matrix = [dictionary.doc2bow(summary) for summary in processed_summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LDA to create summary-topic & topic-word distributions.\n",
    "lda_model = gensim.models.ldamodel.LdaModel(summary_word_matrix, num_topics=3, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.028*\"art\" + 0.028*\"food\" + 0.011*\"right\" + 0.009*\"human\" + 0.009*\"the\" + 0.009*\"skill\" + 0.009*\"organ\" + 0.007*\"gener\" + 0.007*\"intern\" + 0.007*\"world\"\n",
      "Topic 1: 0.018*\"polit\" + 0.017*\"scienc\" + 0.017*\"studi\" + 0.012*\"histori\" + 0.011*\"natur\" + 0.010*\"python\" + 0.009*\"the\" + 0.008*\"geographi\" + 0.007*\"disciplin\" + 0.007*\"use\"\n",
      "Topic 2: 0.028*\"mathemat\" + 0.015*\"natur\" + 0.014*\"knowledg\" + 0.012*\"war\" + 0.010*\"studi\" + 0.010*\"languag\" + 0.007*\"understand\" + 0.007*\"process\" + 0.007*\"practic\" + 0.007*\"gener\"\n"
     ]
    }
   ],
   "source": [
    "# Print some topic-word lists with weightings. \n",
    "# Reference to below 2 lines: https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "for idx, words in lda_model.print_topics(-1):\n",
    "    print('Topic {}: {}'.format(idx, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6019821763038635\t Topic: 0.028*\"mathemat\" + 0.015*\"natur\" + 0.014*\"knowledg\" + 0.012*\"war\" + 0.010*\"studi\"\n",
      "Score: 0.39065396785736084\t Topic: 0.018*\"polit\" + 0.017*\"scienc\" + 0.017*\"studi\" + 0.012*\"histori\" + 0.011*\"natur\"\n"
     ]
    }
   ],
   "source": [
    "# Test a summary of a page is a ~related field to the training data. \n",
    "test_summary_page =  wikipedia.WikipediaPage(title='Cluster analysis').summary\n",
    "# Reference to below 3 lines: https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "bow_vector = dictionary.doc2bow(summary_preprocessing(test_summary_page).split())\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In conclusion, the topic of 'Cluster analysis' is highly related to the suggested topic, classified by words associated with \"mathematics\".\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}